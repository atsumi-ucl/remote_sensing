---
title: "Week 7"
author: "Atsumi Hirose"
editor: visual
---

# Summary

We looked at object-based image analysis which is a hybrid of arts and science, and this was followed by accuracy assessment of classification tasks.Below is a summary.

## Object-based image analysis

The essence of the method is that through iterations, a pixel is turned into a collection of similar pixels, which becomes an object and is used for analysis instead of a pixel. According to Jensen, this method is often used for the analysis of high spatial resolution imagery such as GeoEye-1 or WorldView-2.

## Accuracy assessment

The other thing we looked at is accuracy assessment. Normally a simple two-by-two matrix of true classes and classified classes is used, which I believe is called a 'confusion' matri but Jensen calls it an error matrix [@jensen2014]. Whatever it is called, the two-by-two matrix shows the number of true positives, false positive, true negative and false negative. But for multiclass classification tasks, the number of rows and columns can be equal to the number of classes. *Overall accuracy* can be calculated as the number of labels correctly identified, divided by the total number of labels.

There are other accuracy measures too. *User accuracy* (UA) is calculated as the total number of correctly classified pixels, divided by the total number of pixels classified in that category. UA assess the number \$\$of true positive divided by the number of true positive and false positive. (I guess this is the perspective from the user of the map) *Producer accuracy* is calculated as the total number of correct pixels in a class divided by the total number of pixels of that class from the ground reference data. PA equals the number of true positive divided by true positive and true negative. These are very intuitive.

Another method is to use Kappa. Kappa is given by:

$$
 k = N \sum_{i=1}^n X_ij -
$$

## Validation methods (How do we get the accuracy statistics?)

-   K-fold Cross validation (which uses a different subset of data for testing and take the average accuracy statistics across different test and training splits)

-   Leave one out (this takes one data out from the training dataset of n data points and train the model by using n-1 training data sets) and take average

### What to consider

-   Remember that near things are more related than distant things. This should be taken into account when selecting regions of interests. For validation datasets, pixels too near to the areas used for training datasets should be excluded or a distance should be determined such that the validation data should not be taken from any areas within the distance boundary from the training datasets.
-   Also talked about how to select training datasets and the value of selecting something invariant over time.

# Application

# Reflection
